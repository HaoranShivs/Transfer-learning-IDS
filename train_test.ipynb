{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprocess.kdd_cup99 import KDD_CUP_99_DataLoader\n",
    "from net.CNN_KDD import CNN_KDD\n",
    "from net.multCNN import multCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "log_directory:  /Transfer-learning-IDS/log/exp/multCNN/kddcup_balance/4_21/\n",
      "ckpt_directory:  /Transfer-learning-IDS/history/multCNN/kddcup_balance/checkpoint/4_21/\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "t = time.localtime()\n",
    "year, month, day = t.tm_year, t.tm_mon, t.tm_mday\n",
    "\n",
    "mode = input('input the tag about the training: ')\n",
    "if mode == '':\n",
    "    mode = 'no_tag'\n",
    "log_directory = f\"/Transfer-learning-IDS/log/exp/multCNN/{mode}/{month}_{day}/\"\n",
    "ckpt_directory = f\"/Transfer-learning-IDS/history/multCNN/{mode}/checkpoint/{month}_{day}/\"\n",
    "if not os.path.isdir(log_directory):\n",
    "    os.makedirs(log_directory)\n",
    "if not os.path.isdir(ckpt_directory):\n",
    "    os.makedirs(ckpt_directory)\n",
    "print('log_directory: ', log_directory)\n",
    "print('ckpt_directory: ', ckpt_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KDD_CUP_99_DataLoader('E:/DataSets/kddcup.data', 256)\n",
    "discrete_column_idx = dataset.data.discrete_column\n",
    "discrete_column_idx.remove(41)\n",
    "continuous_column_idx = [i for i in range(41) if i not in discrete_column_idx]\n",
    "input_channel_kind = len(discrete_column_idx)\n",
    "input_channel_num = len(continuous_column_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = CNN_KDD(41,23,3).to(device)\n",
    "net = multCNN(input_channel_num, input_channel_kind, 23).to(device)\n",
    "epoch = 100\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "writer1 = SummaryWriter(log_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('/Transfer-learning-IDS/history/CNN_KDD/checkpoint/ckpt_best_99.pth')\n",
    "# epoch_start = ckpt[\"epoch\"]\n",
    "# net.load_state_dict(ckpt[\"net\"])\n",
    "# optimizer.load_state_dict(ckpt[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, learning rate: 0.100000\n",
      "epoch: 1, learning rate: 0.100000\n",
      "epoch: 2, learning rate: 0.100000\n",
      "epoch: 3, learning rate: 0.100000\n",
      "epoch: 4, learning rate: 0.100000\n",
      "epoch: 5, learning rate: 0.100000\n",
      "epoch: 6, learning rate: 0.100000\n",
      "epoch: 7, learning rate: 0.100000\n",
      "epoch: 8, learning rate: 0.100000\n",
      "epoch: 9, learning rate: 0.100000\n",
      "epoch: 10, learning rate: 0.100000\n",
      "epoch: 11, learning rate: 0.100000\n",
      "epoch: 12, learning rate: 0.100000\n",
      "epoch: 13, learning rate: 0.100000\n",
      "epoch: 14, learning rate: 0.100000\n",
      "epoch: 15, learning rate: 0.100000\n",
      "epoch: 16, learning rate: 0.100000\n",
      "epoch: 17, learning rate: 0.100000\n",
      "epoch: 18, learning rate: 0.100000\n",
      "epoch: 19, learning rate: 0.100000\n",
      "epoch: 20, learning rate: 0.025000\n",
      "epoch: 21, learning rate: 0.025000\n",
      "epoch: 22, learning rate: 0.025000\n",
      "epoch: 23, learning rate: 0.025000\n",
      "epoch: 24, learning rate: 0.025000\n",
      "epoch: 25, learning rate: 0.025000\n",
      "epoch: 26, learning rate: 0.025000\n",
      "epoch: 27, learning rate: 0.025000\n",
      "epoch: 28, learning rate: 0.025000\n",
      "epoch: 29, learning rate: 0.025000\n",
      "epoch: 30, learning rate: 0.025000\n",
      "epoch: 31, learning rate: 0.025000\n",
      "epoch: 32, learning rate: 0.025000\n",
      "epoch: 33, learning rate: 0.025000\n",
      "epoch: 34, learning rate: 0.025000\n",
      "epoch: 35, learning rate: 0.025000\n",
      "epoch: 36, learning rate: 0.025000\n",
      "epoch: 37, learning rate: 0.025000\n",
      "epoch: 38, learning rate: 0.025000\n",
      "epoch: 39, learning rate: 0.025000\n",
      "epoch: 40, learning rate: 0.006250\n",
      "epoch: 41, learning rate: 0.006250\n",
      "epoch: 42, learning rate: 0.006250\n",
      "epoch: 43, learning rate: 0.006250\n",
      "epoch: 44, learning rate: 0.006250\n",
      "epoch: 45, learning rate: 0.006250\n",
      "epoch: 46, learning rate: 0.006250\n",
      "epoch: 47, learning rate: 0.006250\n",
      "epoch: 48, learning rate: 0.006250\n",
      "epoch: 49, learning rate: 0.006250\n",
      "epoch: 50, learning rate: 0.006250\n",
      "epoch: 51, learning rate: 0.006250\n",
      "epoch: 52, learning rate: 0.006250\n",
      "epoch: 53, learning rate: 0.006250\n",
      "epoch: 54, learning rate: 0.006250\n",
      "epoch: 55, learning rate: 0.006250\n",
      "epoch: 56, learning rate: 0.006250\n",
      "epoch: 57, learning rate: 0.006250\n",
      "epoch: 58, learning rate: 0.006250\n",
      "epoch: 59, learning rate: 0.006250\n",
      "epoch: 60, learning rate: 0.001563\n",
      "epoch: 61, learning rate: 0.001563\n",
      "epoch: 62, learning rate: 0.001563\n",
      "epoch: 63, learning rate: 0.001563\n",
      "epoch: 64, learning rate: 0.001563\n",
      "epoch: 65, learning rate: 0.001563\n",
      "epoch: 66, learning rate: 0.001563\n",
      "epoch: 67, learning rate: 0.001563\n",
      "epoch: 68, learning rate: 0.001563\n",
      "epoch: 69, learning rate: 0.001563\n",
      "epoch: 70, learning rate: 0.001563\n",
      "epoch: 71, learning rate: 0.001563\n",
      "epoch: 72, learning rate: 0.001563\n",
      "epoch: 73, learning rate: 0.001563\n",
      "epoch: 74, learning rate: 0.001563\n",
      "epoch: 75, learning rate: 0.001563\n",
      "epoch: 76, learning rate: 0.001563\n",
      "epoch: 77, learning rate: 0.001563\n",
      "epoch: 78, learning rate: 0.001563\n",
      "epoch: 79, learning rate: 0.001563\n",
      "epoch: 80, learning rate: 0.000391\n",
      "epoch: 81, learning rate: 0.000391\n",
      "epoch: 82, learning rate: 0.000391\n",
      "epoch: 83, learning rate: 0.000391\n",
      "epoch: 84, learning rate: 0.000391\n",
      "epoch: 85, learning rate: 0.000391\n",
      "epoch: 86, learning rate: 0.000391\n",
      "epoch: 87, learning rate: 0.000391\n",
      "epoch: 88, learning rate: 0.000391\n",
      "epoch: 89, learning rate: 0.000391\n",
      "epoch: 90, learning rate: 0.000391\n",
      "epoch: 91, learning rate: 0.000391\n",
      "epoch: 92, learning rate: 0.000391\n",
      "epoch: 93, learning rate: 0.000391\n",
      "epoch: 94, learning rate: 0.000391\n",
      "epoch: 95, learning rate: 0.000391\n",
      "epoch: 96, learning rate: 0.000391\n",
      "epoch: 97, learning rate: 0.000391\n",
      "epoch: 98, learning rate: 0.000391\n",
      "epoch: 99, learning rate: 0.000391\n"
     ]
    }
   ],
   "source": [
    "for t in range(0, epoch):\n",
    "    running_loss = 0\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        \n",
    "        x_continuous, x_discrete =x[:, continuous_column_idx], x[:, discrete_column_idx]\n",
    "        x_continuous = x_continuous.to(device)\n",
    "        x_discrete = x_discrete.to(device)\n",
    "        y = F.one_hot(y.long(), num_classes=23).float()\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = net((x_continuous, x_discrete))\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # visualize loss\n",
    "        running_loss += loss.item()\n",
    "    # ...log the running loss\n",
    "    writer1.add_scalar('training loss', running_loss, t)\n",
    "    print(\"epoch: %d, learning rate: %f\" % (t, optimizer.param_groups[0]['lr']))\n",
    "    scheduler.step()\n",
    "    if t % 10 == 9:\n",
    "        checkpoint = {\"net\": net.state_dict(), 'optimizer':optimizer.state_dict(), \"epoch\": t}\n",
    "        torch.save(checkpoint,  ckpt_directory + 'ckpt_best_%s.pth' %(str(t)))\n",
    "writer1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6012523c976ef1bcce85e119c3469a3f8f1d8cb66ef0d8b93cf00e121cb3e03"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('py37@pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
